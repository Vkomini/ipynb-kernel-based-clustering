{
 "metadata": {
  "name": "",
  "signature": "sha256:4ca26550bea91557d4939099525cd1629b8aab2bffcee16f53b9a176957e6c93"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Cheat sheet on kernel-based clustering\n",
      "\n",
      "---\n",
      "Clustering methods such as spectral clustering or weighted kernel k-means are popular algorithms for unsupervised learning. Since implementations are widely available they are easy to use and utilizing the kernel method makes them applicable to many problems, not only linear ones. However there are several variations and difficulties that sometimes require to take a second look at what is actually going on within the clustering algorithm. This notebook is intended as cheat sheet for quick reference. For detailed explanation the interested reader is refered to the list of literature referenced at [the end](#dhillon)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Overview $\\newcommand\\v[1]{\\boldsymbol{#1}} \\newcommand\\s[1]{\\mathcal{#1}}$\n",
      "\n",
      "---\n",
      "\n",
      "The main advantage of kernel-based methods is that they allow us to design the problem directly in terms of data point similarities instead of transforming points to some abstract feature space. With respect to the clustering problem we can simply define a function that measures how equal a data pair is and use this information to decide on whether it belongs to the same cluster or not. Given a data set $\\s V = \\{\\v x_i\\}_i^n$ of $n$ data points, we construct a kernel $\\v K \\in \\mathbb{R}^{n\\times n}$ by using the kernel function $\\v K_{ij} = \\kappa(\\v x_i,\\v x_j)$. Some commonly used:\n",
      "\n",
      "- **Polynomial:**\n",
      "  > $\\kappa(\\v x_i,\\v x_j) = (\\v x_i^T \\v x_j + c)^d$\n",
      "  \n",
      "- **Gaussian:**\n",
      "  > $\\kappa(\\v x_i,\\v x_j)=\\exp(-\\frac{1}{2\\sigma^2}\\|\\v x_i-\\v x_j\\|^2)$\n",
      "\n",
      "- **Sigmoid:**\n",
      "  >$\\kappa(\\v x_i,\\v x_j) = \\tanh(c(\\v x_i^T \\v x_j) +\\theta)$\n",
      "\n",
      "\n",
      "Performing clustering on a kernel matrix $\\v K$ can also be viewed from the graph partitioning perspective. Given graph $G = (\\s V,\\s E)$, each node in $\\s V$ resembles one data point and a each edge in $\\s E$ the similarity between two nodes defined by our kernel function $\\kappa(\\v x_i,\\v x_j)$. Another noticable matrix when working with graphs is the diagonal degree matrix $\\v D$ with entries $\\v D_{ii} = \\sum_{j=1}^n \\kappa(\\v x_i,\\v x_j)$.\n",
      "One can further define the similarity of two sets of nodes as \n",
      "\n",
      "$$ \\delta(\\s A,\\s B) = \\sum_{i \\in \\s A, j \\in \\s B} \\kappa(\\v x_i,\\v x_j)$$\n",
      "\n",
      "which is the sum of similarities that connects both sets. We can name\n",
      "a few special cases:\n",
      "\n",
      "\n",
      "- **Association:**\n",
      "  > $\\delta(\\s V_c, \\s V_c)$\n",
      "\n",
      "- **Cut:**\n",
      "  > $\\delta(\\s V_c, \\s V\\backslash \\s V_c)$\n",
      " \n",
      "- **Degree:**\n",
      "  > $\\delta(\\s V_c, \\s V)$\n",
      "\n",
      "- **Association + Cut = Degree:**\n",
      "  > $\\delta(\\s V_c, \\s V_c) + \\delta(\\s V_c, \\s V\\backslash \\s V_c) = \\delta(\\s V_c, \\s V)$\n",
      "\n",
      "\n",
      "The goal is to separate the nodes into $k$ clusters by finding a set \n",
      "of indicator vectors $\\{\\v y_c\\}_1^k$ that determine whether a node belongs to the corresponding cluster ($\\v y_c(i) = 1$).\n",
      "The most common objectives are:\n",
      "\n",
      "- **Average Association** which seeks to maximize the within-cluster similarity. Regular kernel $k$-means follows this objective.\n",
      "\n",
      "  $$\\max\\left\\{ \n",
      "  \\sum_{c=1}^k \\frac{\\delta(\\s V_c,\\s V_c)}{|\\s V_c|} \n",
      "  = \\sum_{c=1}^k \\frac{\\v y_c^T \\v K \\v y_c}{\\v y_c^T\\v y_c}\n",
      "  \\right\\}$$\n",
      "  \n",
      "  ---\n",
      "\n",
      "- **Average Cut** which attempts to minimize the similarites between clusters. This is the objective used by spectral clustering with the graph laplacian $\\v L=\\v D-\\v K$.\n",
      "\n",
      "  $$\\min\\left\\{\n",
      "  \\sum_{c=1}^k \\frac{\\delta(\\s V_c,\\s V\\backslash\\s V_c)}{|\\s V_c|}\n",
      "  = \\sum_{c=1}^k \\frac{\\v y_c^T (\\v D-\\v K) \\v y_c}{\\v y_c^T\\v y_c}\n",
      "  \\right\\}$$\n",
      "  \n",
      " ---\n",
      "  \n",
      "- **Normalized Cut** which places itself in between avg. cut and avg. association in order to achieve a trade-off of both objectives. Spectral clustering with $\\v L = \\v I -\\v D^{-1/2}\\v K\\v D^{-1/2}$ and weighted kernel $k$-means use this.\n",
      "\n",
      "  $$\\min\\left\\{\n",
      "  \\sum_{c=1}^k \\frac{\\delta(\\s V_c,\\s V\\backslash\\s V_c)}{\\delta(\\s V_c,\\s V)}\n",
      "  = \\sum_{c=1}^k \\frac{\\v y_c^T (\\v D-\\v K) \\v y_c}{\\v y_c^T \\v D\\v y_c}\n",
      "  \\right\\}$$\n",
      "  \n",
      "  ---\n",
      "  \n",
      "- **Normalized Association** which is identical to normalized cuts but formulated as maximization:\n",
      "\n",
      "  $$\\max\\left\\{\n",
      "  \\sum_{c=1}^k \\frac{\\delta(\\s V_c,\\s V_c)}{\\delta(\\s V_c,\\s V)}\n",
      "  = \\sum_{c=1}^k \\frac{\\v y_c^T \\v K \\v y_c}{\\v y_c^T \\v D\\v y_c}\n",
      "  \\right\\}$$\n",
      "  \n",
      "  ---\n",
      "  \n",
      "All objectives can also be written in trace maximization form and constraint that $\\v Y^T\\v Y = \\v I_k$ is orthonormal:\n",
      "$\\newcommand\\trace[1]{\\mathrm{Tr}\\left\\{#1\\right\\}}$\n",
      "- **AAssoc:**\n",
      "  $$\\max\\; \\trace{\\v Y^T \\v K \\v Y}$$\n",
      "\n",
      "\n",
      "- **ACut:**\n",
      "  $$\\max\\; \\trace{\\v Y^T(\\v I_n-(\\v D-\\v K))-\\v Y}$$\n",
      "\n",
      "\n",
      "- **NCut/NAssoc:**\n",
      "  $$\\max\\; \\trace{\\v Z^T \\v D^{-1/2}\\v K\\v D^{-1/2}\\v Z}$$\n",
      "  \n",
      "  with $\\v Z = \\v D^{1/2}\\v Y$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Reference\n",
      "<a id='dhillon'/></a>[1]: I. Dhillon, Y. Guan, and B. Kulis, *A unified view of kernel k-means, spectral clustering and graph cuts*, 2004.\n",
      "\n",
      "<a id='luxburg'/></a>[2]: U. von Luxburg, *A tutorial on spectral clustering*, 2007.\n",
      "\n",
      "<a id='bach'/></a>[3]: F. R. Bach and M. I. Jordan, *Learning Spectral Clustering*, 2003.\n",
      "\n",
      "<a id='chi'/></a>[4]: Y. Chi, X. Song, D. Zhou, K. Hino, and B. L. Tseng, *Evolutionary Spectral Clustering by Incorporating Temporal Smoothness*, 2007."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}